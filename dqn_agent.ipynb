{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import numpy as np                  # Numpy numerical library\r\n",
    "import pandas as pd                 # Pandas for dataframes manipulation\r\n",
    "import tensorflow as tf             # TensorFlow for neural networks and deep learning APIs\r\n",
    "import matplotlib.pyplot as plt     # Plotting\r\n",
    "%matplotlib inline                  # Magic formula for using plots with jupyter notebooks"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "BATCH_SIZE = 4              # Default batch size\r\n",
    "\r\n",
    "class DataLoader:\r\n",
    "    # Take in a csv file and extracts features and labels\r\n",
    "    def __init__(self, csv_filepath, batch_size = BATCH_SIZE):                          \r\n",
    "        \r\n",
    "        self.df_samples = pd.read_csv(csv_filepath)                 # Create a pandas dataframe\r\n",
    "        self.numpy_samples = self.df_samples.to_numpy()\r\n",
    "        \r\n",
    "        self.states_features = self.numpy_samples[:, 1:self.numpy_samples.shape[1]-1]               # Take the feature values for states, also ignore first column for IDs  \r\n",
    "        self.feature_dim = self.states_features.shape[1]                                        \r\n",
    "        self.actions_labels = self.numpy_samples[:, -1].reshape(-1, 1)                              # The action labels separated from the labels\r\n",
    "        self.actions_classes = np.amax(self.actions_labels) + 1                                     # Number of different action classes to set the output layer dimensions (+1 bec starts at zero)\r\n",
    "        self.batches = self.prepare_batches(batch_size)\r\n",
    "        \r\n",
    "    def prepare_batches(self, batch_size):\r\n",
    "        states_t = self.states_features[:-1, :]           # Considered as S(t)\r\n",
    "        states_t_plus_one = self.states_features[1:, :]   # Considered as S(t+1)\r\n",
    "        actions_star_t = self.actions_labels[:-1, :]      # Considered as a*(t)\r\n",
    "        whole_generic_samples = np.hstack((states_t, actions_star_t, states_t_plus_one))            # Stack the whole dataset as described in the paper\r\n",
    "        \r\n",
    "        self.n_samples = whole_generic_samples.shape[0]\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.n_batches = ceil(self.n_samples / self.batch_size)\r\n",
    "\r\n",
    "        batches = []\r\n",
    "\r\n",
    "        for i in range(self.n_batches):\r\n",
    "            start = i * batch_size\r\n",
    "            end = (i + 1) * batch_size + 1 \r\n",
    "            curr_batch = whole_generic_samples[start:end, :]\r\n",
    "\r\n",
    "        print()\r\n",
    "        return batches\r\n",
    "    \r\n",
    "    # Function that takes in the 2D arrays of data and converts lo lists of tuples to be compatible with looping while training\r\n",
    "    def tupelize(self, array):\r\n",
    "        list_of_tuples = list(zip(array.T[0], array.T))\r\n",
    "        return list_of_tuples \r\n",
    "\r\n",
    "    # Function to get the unique rows representing unique states, returns a numpy array of rows\r\n",
    "    def get_unique_rows(self):\r\n",
    "        self.unique_rows = np.unique(self.states_features, axis = 0)\r\n",
    "        return self.unique_rows\r\n",
    "\r\n",
    "    # Get the pandas dataframe for the data, returns a pandas dataframe\r\n",
    "    def get_dataframe(self):    \r\n",
    "        return self.df_samples\r\n",
    "\r\n",
    "data = DataLoader(\"new_data.csv\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Classes Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "Q_OUT_DIM = 1\r\n",
    "LEARNING_RATE = 0.001       # Gradient-descent learning rate\r\n",
    "REPLAY_MEMORY_SIZE = 50     # Size for RL replay memory\r\n",
    "UPDATE_TARGET_EVERY = 5     \r\n",
    "\r\n",
    "# Creating our main class for our DQN\r\n",
    "class DeepQNet:\r\n",
    "    \r\n",
    "    def __init__(self, dataset):\r\n",
    "        self.data = dataset                                         # Storing the data in our QNet            \r\n",
    "        self.input_dim = dataset.feature_dim + 1                    # State feature dim + 1 (for ground truth actions)\r\n",
    "        self.output_dim = Q_OUT_DIM                               \r\n",
    "        \r\n",
    "        self.model = self.create_model()                            # Main model that gets trained every step \r\n",
    "        self.model.summary()                                        # Printing model details\r\n",
    "        self.target_model = self.create_model()                     # Target model we predict against each step\r\n",
    "        self.target_model.set_weights(self.model.get_weights())     # To make all the initial weights the same\r\n",
    "\r\n",
    "        # Defining the action-set\r\n",
    "        self.action_set = np.arange(self.output_dim).astype(np.uint16).tolist() # List of all possible actions [0, 1, ... output_dim]\r\n",
    "\r\n",
    "        # Used to count when to update target network with main network's weights\r\n",
    "        self.target_update_counter = 0\r\n",
    "    \r\n",
    "    def create_model(self):\r\n",
    "        # Definition of the neural network architecture mentioned in the paper (3 relu feedforward layers)\r\n",
    "        model = tf.keras.Sequential()\r\n",
    "        model.add(tf.keras.layers.Input(self.input_dim))                        # Input dimension of the state-vector\r\n",
    "        model.add(tf.keras.layers.Dense(128, activation= \"relu\"))\r\n",
    "        model.add(tf.keras.layers.Dense(128, activation= \"relu\"))\r\n",
    "        model.add(tf.keras.layers.Dense(self.output_dim, activation= \"relu\"))       # Output is value function\r\n",
    "        model.compile(loss=\"mse\", optimizer=tf.optimizers.Adam(lr= LEARNING_RATE), metrics=['accuracy'])\r\n",
    "        return model\r\n",
    "\r\n",
    "    def get_reward(self, a_predicted, a_label):\r\n",
    "        if a_predicted == a_label:\r\n",
    "            return 1\r\n",
    "        else: return 0\r\n",
    "\r\n",
    "    # Function to implement the epsilon-greedy policy selection, returns the index of the selected action\r\n",
    "    def greedy(self, epsilon, action_values):\r\n",
    "        p = np.random.uniform(low=0.0, high=1.0)\r\n",
    "        \r\n",
    "        if p < epsilon:             # Take the greedy action\r\n",
    "            return np.argmax(action_values)            \r\n",
    "        \r\n",
    "        else:                       # Take an exploration action\r\n",
    "            return np.random.randint(low=0, high=len(action_values))\r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        \r\n",
    "        batches = self.data.batches             # Get the batches\r\n",
    "        self.target_update_counter = 0          # The update counter \r\n",
    "\r\n",
    "        for current_states, optimal_actions, next_states in batches:\r\n",
    "            estimated_qs_list = []\r\n",
    "            for action in self.action_set:                  # Iterating and calculating the value for each action\r\n",
    "                action_vector = np.full((current_states.shape[0], 1), fill_value= action)\r\n",
    "                input_vector =  np.hstack((current_states, action_vector))\r\n",
    "                estimated_qs_list.append(self.target_model.predict(input_vector))\r\n",
    "\r\n",
    "            # If counter reaches set value, update target network with weights of main network\r\n",
    "            if self.target_update_counter > UPDATE_TARGET_EVERY:\r\n",
    "                self.target_model.set_weights(self.model.get_weights())\r\n",
    "                self.target_update_counter = 0\r\n",
    "                \r\n",
    "\r\n",
    "              \r\n",
    "\r\n",
    "dq_net = DeepQNet(dataset= data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               3584      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 21,128\n",
      "Trainable params: 21,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Testing and Evaluation"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "eded81c4a7c6917c9cbd3629f4297c0af6b02e3629b6d4cad1fc0bc42eaeccd9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}