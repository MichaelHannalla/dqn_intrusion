{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Library Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import random\r\n",
    "import numpy as np                  # Numpy numerical library\r\n",
    "import pandas as pd                 # Pandas for dataframes manipulation\r\n",
    "import tensorflow as tf             # TensorFlow for neural networks and deep learning APIs\r\n",
    "import matplotlib.pyplot as plt     # Plotting\r\n",
    "from sklearn.metrics import precision_score, recall_score # Importing performance metrics calculators\r\n",
    "%matplotlib inline                  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "BATCH_SIZE = 64                 # Default batch size\r\n",
    "TRAIN_SPLIT_PERCENT = 0.8       # Percentage of the data for training, rest for testing  \r\n",
    "\r\n",
    "class DataLoader:\r\n",
    "    # Take in a csv file and extracts features and labels\r\n",
    "    def __init__(self, csv_filepath, batch_size):                          \r\n",
    "        \r\n",
    "        self.df_samples = pd.read_csv(csv_filepath)                 # Create a pandas dataframe\r\n",
    "        self.numpy_samples = self.df_samples.to_numpy()\r\n",
    "        \r\n",
    "        self.states_features = self.numpy_samples[:, 1:self.numpy_samples.shape[1]-1]               # Take the feature values for states, also ignore first column for IDs  \r\n",
    "        self.feature_dim = self.states_features.shape[1]                                        \r\n",
    "        self.actions_labels = self.numpy_samples[:, -1].reshape(-1, 1)                              # The action labels separated from the labels\r\n",
    "        self.actions_classes = np.amax(self.actions_labels) + 1                                     # Number of different action classes to set the output layer dimensions (+1 bec starts at zero)\r\n",
    "        self.actions_set = np.arange(self.actions_classes).tolist()                                 # Set of all possible actions \r\n",
    "        self.train_batches, self.test_batches = self.prepare_batches(batch_size, train_split_percent= TRAIN_SPLIT_PERCENT)\r\n",
    "        print(\"Dataset successfully loaded with {} training batches, and {} testing batches with {} batch size.\".format(\r\n",
    "            len(self.train_batches), len(self.test_batches), self.batch_size\r\n",
    "        ))\r\n",
    "        \r\n",
    "    def prepare_batches(self, batch_size, train_split_percent):\r\n",
    "        states_t = self.states_features[:-1, :].copy()           # Considered as S(t)\r\n",
    "        states_t_plus_one = self.states_features[1:, :].copy()   # Considered as S(t+1)\r\n",
    "        actions_star_t = self.actions_labels[:-1, :].copy()      # Considered as a*(t)\r\n",
    "\r\n",
    "        whole_generic_samples = np.hstack((states_t, actions_star_t, states_t_plus_one))            # Stack the whole dataset as described in the paper\r\n",
    "\r\n",
    "        np.random.seed(0)                                   # To shuffle similarly each time, comment to disable this feature\r\n",
    "        np.random.shuffle(whole_generic_samples)            # Shuffle the dataset\r\n",
    "        \r\n",
    "        self.n_samples = whole_generic_samples.shape[0]\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.n_batches = np.ceil(self.n_samples / self.batch_size).astype(np.uint32)\r\n",
    "\r\n",
    "        train_batches = []        # Empty list to hold the batches of whole data\r\n",
    "        test_batches = []\r\n",
    "\r\n",
    "        # Prepare the data into batches\r\n",
    "        for i in range(self.n_batches):\r\n",
    "            start = i * batch_size\r\n",
    "            end = (i + 1) * batch_size\r\n",
    "            curr_batch = whole_generic_samples[start:end, :]\r\n",
    "            if (i / self.n_batches) < train_split_percent:    \r\n",
    "                train_batches.append(curr_batch)\r\n",
    "            else:\r\n",
    "                test_batches.append(curr_batch)\r\n",
    "        \r\n",
    "        self.n_train_batches = len(train_batches)\r\n",
    "        self.n_test_batches = len(test_batches)\r\n",
    "        return train_batches, test_batches\r\n",
    "    \r\n",
    "    # Function that takes in the 2D arrays of data and converts lo lists of tuples to be compatible with looping while training\r\n",
    "    # TODO: Enhancing these function (All the following isn't used)\r\n",
    "    def tupelize(self, array):\r\n",
    "        list_of_tuples = list(zip(array.T[0], array.T))\r\n",
    "        return list_of_tuples \r\n",
    "\r\n",
    "    # Function to get the unique rows representing unique states, returns a numpy array of rows\r\n",
    "    def get_unique_rows(self):\r\n",
    "        self.unique_rows = np.unique(self.states_features, axis = 0)\r\n",
    "        return self.unique_rows\r\n",
    "\r\n",
    "    # Get the pandas dataframe for the data, returns a pandas dataframe\r\n",
    "    def get_dataframe(self):    \r\n",
    "        return self.df_samples\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Classes Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "LEARNING_RATE = 0.001    # Gradient-descent learning rate    \r\n",
    "EPSILON = 0.8            # Epsilon value for the epsilon greedy policy selection\r\n",
    "LAMBDA = 0.01            # Discount factor for loss calculation\r\n",
    "EPOCHS = 10              # Number of training epochs\r\n",
    "\r\n",
    "# Creating our main class for our DQN\r\n",
    "class DeepQNet:\r\n",
    "    \r\n",
    "    def __init__(self, dataset):\r\n",
    "        self.data = dataset                                         # Storing the data in our QNet            \r\n",
    "        self.input_dim = dataset.feature_dim                        # State feature dim\r\n",
    "        self.output_dim = dataset.actions_classes\r\n",
    "        \r\n",
    "        self.model = self.create_model()                            # Main DQN model\r\n",
    "\r\n",
    "        # Used to count when to update target network with main network's weights\r\n",
    "        self.target_update_counter = 0\r\n",
    "    \r\n",
    "    def create_model(self):\r\n",
    "        # Definition of the neural network architecture mentioned in the paper (3 relu feedforward layers)\r\n",
    "        model = tf.keras.Sequential()\r\n",
    "        model.add(tf.keras.layers.Input(self.input_dim))                                # Input dimension of the state-vector\r\n",
    "        model.add(tf.keras.layers.Dense(32, activation= \"relu\"))\r\n",
    "        model.add(tf.keras.layers.Dense(32, activation= \"relu\"))\r\n",
    "        model.add(tf.keras.layers.Dense(self.output_dim, activation= \"softmax\"))        # Output is value function selection\r\n",
    "        model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.optimizers.Adam(lr= LEARNING_RATE, clipnorm=1.0), metrics=['accuracy'])\r\n",
    "        return model\r\n",
    "\r\n",
    "    def load_model(self, path):\r\n",
    "        self.model = tf.keras.models.load_model(path)\r\n",
    "\r\n",
    "    # Prints the model details\r\n",
    "    def summary(self):\r\n",
    "        self.model.summary()\r\n",
    "\r\n",
    "    def get_reward(self, predicted_actions, optimal_actions):\r\n",
    "        predicted_actions = np.asarray(predicted_actions).reshape(-1)\r\n",
    "        optimal_actions = np.asarray(optimal_actions).reshape(-1)\r\n",
    "        reward_vector = np.equal(predicted_actions, optimal_actions).astype(np.uint32)\r\n",
    "        return reward_vector\r\n",
    "\r\n",
    "    # Function to implement the epsilon-greedy policy selection, returns the index of the selected action\r\n",
    "    def greedy(self, actions_values_vec, epsilon):\r\n",
    "        random.seed(a=None, version=2)               # Change the seed of randomization\r\n",
    "        num_in_curr_batch = actions_values_vec.shape[0]\r\n",
    "        selections = []\r\n",
    "        for i in range(num_in_curr_batch):\r\n",
    "            p = random.uniform(0.0, 1.0)\r\n",
    "            if p < epsilon:\r\n",
    "                curr_actions_values = actions_values_vec[i, :].reshape(-1)\r\n",
    "                selections.append(np.argmax(curr_actions_values))\r\n",
    "            else:\r\n",
    "                random_selection = np.random.randint(low=0, high=self.data.actions_classes)\r\n",
    "                selections.append(random_selection)\r\n",
    "        \r\n",
    "        return np.asarray(selections)\r\n",
    "\r\n",
    "    # Function to process the batch and split the S(t), a*(t), and S(t+1)\r\n",
    "    def process_batch(self, batch):\r\n",
    "        current_states = batch[:, :self.data.feature_dim]\r\n",
    "        optimal_actions = batch[:, self.data.feature_dim].astype(np.uint32)\r\n",
    "        next_states = batch[:, self.data.feature_dim+1 :]\r\n",
    "        return current_states, optimal_actions, next_states\r\n",
    "\r\n",
    "    def train(self, save_path=None):\r\n",
    "        \r\n",
    "        losses = []\r\n",
    "        accuracies = []\r\n",
    "        batches = self.data.train_batches               # Get the batches \r\n",
    "\r\n",
    "        for epoch in range(EPOCHS):\r\n",
    "\r\n",
    "            for batch_idx, batch in enumerate(batches):                   # Looping over the batches\r\n",
    "                current_states, optimal_actions, next_states = self.process_batch(batch)\r\n",
    "\r\n",
    "                # Prediction on S(t)\r\n",
    "                estimated_qs_vec_t = self.model.predict(current_states)\r\n",
    "                predicted_actions_t = self.greedy(estimated_qs_vec_t, epsilon= EPSILON)             # Predict the actions based on epsilon-greedy algorithm\r\n",
    "                rewards_t = self.get_reward(predicted_actions_t, optimal_actions)                   # Get the reward for each sample, the variable here unused becuase the rewarding phenomenon\r\n",
    "                                                                                                    # is already done implicitly down while putting 1's in the q_values of optimal actions\r\n",
    "                \r\n",
    "                # Prediction on S(t+1)\r\n",
    "                estimated_qs_vec_t_plus_one = self.model.predict(next_states)\r\n",
    "                predicted_actions_t_plus_one = self.greedy(estimated_qs_vec_t_plus_one, epsilon= 1.0)     # Taking the always argmax (epsilon = 1.0)\r\n",
    "                \r\n",
    "                # An np.arange object to access all rows, for vectorization\r\n",
    "                all_rows_idx = np.arange(estimated_qs_vec_t.shape[0])\r\n",
    "\r\n",
    "                # Prediction with S(t+1) and a_cap(t+1)\r\n",
    "                q_cap_t_plus_one = self.model.predict(next_states)[all_rows_idx, predicted_actions_t_plus_one]  # Getting the q_values for the next predicted actions\r\n",
    "                \r\n",
    "                # Calculation of qref\r\n",
    "                qref = np.zeros_like(estimated_qs_vec_t)                                            # Set the qref shape and initialize as zeros\r\n",
    "                qref[all_rows_idx, optimal_actions] = 1                                             # Setting 1 to all values that correspond to the action of maximum value.\r\n",
    "                qref[all_rows_idx, predicted_actions_t] += LAMBDA * q_cap_t_plus_one                # qref = rt + qcap_t+1\r\n",
    "                qref_softmax = np.zeros_like(qref)                                                  # Softmax here is just for intuition, while what we do here is a hard max.\r\n",
    "                qref_softmax[all_rows_idx, qref.argmax(1)] = 1                                      # Replacet the max value of the function by 1, all others by zeros. To act like classifier\r\n",
    "                \r\n",
    "                # This line bypasses the RL exploration, makes the agent works as an ordinal NN (uncomment if you want this to happen.)\r\n",
    "                #qref_softmax = tf.keras.utils.to_categorical(optimal_actions)\r\n",
    "\r\n",
    "                loss, accuracy = self.model.train_on_batch(current_states, qref_softmax, reset_metrics= False)\r\n",
    "                losses.append(loss)\r\n",
    "                accuracies.append(accuracy)\r\n",
    "                #self.model.fit(current_states, qref_softmax)\r\n",
    "                #continue\r\n",
    "            \r\n",
    "                print(\" -------------------------------------------------- \")\r\n",
    "                print(\"In epoch {}/{} epochs, batch {}/{} batches:\".format(epoch+1, EPOCHS, batch_idx+1, self.data.n_train_batches))\r\n",
    "                print(\"Accuracy: {}\".format(accuracy))\r\n",
    "                print(\"Loss: {}\".format(loss))\r\n",
    "                print(\" -------------------------------------------------- \")\r\n",
    "        \r\n",
    "        if save_path != None:\r\n",
    "            self.model.save(save_path)\r\n",
    "\r\n",
    "        return losses, accuracies\r\n",
    " \r\n",
    "    def test(self):\r\n",
    "        batches = self.data.test_batches\r\n",
    "        \r\n",
    "        accuracy = 0\r\n",
    "        f1_score = 0\r\n",
    "        precision = 0\r\n",
    "        recall = 0\r\n",
    "\r\n",
    "        for batch_idx, batch in enumerate(batches):                             # Looping over the batches\r\n",
    "            current_states, optimal_actions, _ = self.process_batch(batch)      # Get the data from the batch\r\n",
    "            estimated_qs_vec = self.model.predict(current_states)\r\n",
    "            predicted_actions = self.greedy(estimated_qs_vec, epsilon= 1.0).squeeze()     # Since we are testing so we need no exploration, we are only greedy now (eps=1.0)\r\n",
    "            #print(np.unique(predicted_actions))\r\n",
    "\r\n",
    "            curr_batch_accuracy = np.mean(np.equal(predicted_actions, optimal_actions).astype(np.uint32)) \r\n",
    "            curr_batch_precision = precision_score(optimal_actions, predicted_actions, labels= self.data.actions_set, average=\"weighted\", zero_division= 1)\r\n",
    "            curr_batch_recall = recall_score(optimal_actions, predicted_actions, labels= self.data.actions_set, average=\"weighted\", zero_division= 1)\r\n",
    "            curr_batch_f1 = 2*((curr_batch_precision*curr_batch_recall)/(curr_batch_precision+curr_batch_recall+1e-7))\r\n",
    "\r\n",
    "            accuracy += curr_batch_accuracy / len(batches)\r\n",
    "            precision += curr_batch_precision / len(batches) \r\n",
    "            recall += curr_batch_recall / len(batches)\r\n",
    "            f1_score += curr_batch_f1 / len(batches)\r\n",
    "        \r\n",
    "        print(\"Finished testing on the testing dataset, now printing metrics.\")\r\n",
    "        print(\"Accuracy: {}\".format(accuracy))\r\n",
    "        print(\"Precision: {}\".format(precision))\r\n",
    "        print(\"Recall: {}\".format(recall))\r\n",
    "        print(\"F1 Score: {}\".format(f1_score))\r\n",
    "        return accuracy, precision, recall, f1_score\r\n",
    "\r\n",
    "    def predict(self, states):\r\n",
    "        estimated_qs = self.model.predict(states)\r\n",
    "        predicted_action = self.greedy(estimated_qs, epsilon=1.0).squeeze()\r\n",
    "        return predicted_action\r\n",
    "\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training our model\r\n",
    "data = DataLoader(\"data/ids_data.csv\", batch_size= BATCH_SIZE)              # Importing the dataset using our dataloader\r\n",
    "dq_net = DeepQNet(dataset= data)                                            # Creating our DQNet\r\n",
    "dq_net.summary()                                                            # Printing the model contents\r\n",
    "losses, accuracies = dq_net.train(save_path= \"models/ids_dqnet.ckpt\")       # Calling the train function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Performance Curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Using plotting techniques to visualize the performance curves of training our model.\r\n",
    "plt.title(\"Losses vs Batch Step\")\r\n",
    "plt.plot(losses)\r\n",
    "plt.figure()\r\n",
    "plt.title(\"Accuracy vs Batch Step\")\r\n",
    "plt.plot(accuracies)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Testing and Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Testing our model\r\n",
    "data = DataLoader(\"data/ids_data.csv\", batch_size= BATCH_SIZE)              # Importing the dataset using our dataloader\r\n",
    "dq_net = DeepQNet(dataset= data)                                            # Creating our DQNet\r\n",
    "dq_net.load_model(\"models/ids_dqnet.ckpt\")\r\n",
    "dq_net.test()   "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset successfully loaded with 1213 training batches, and 303 testing batches with 64 batch size.\n",
      "Finished testing on the testing dataset, now printing metrics.\n",
      "Accuracy: 0.972498435188347\n",
      "Precision: 0.9751805508837553\n",
      "Recall: 0.972498435188347\n",
      "F1 Score: 0.9738337437820925\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.972498435188347, 0.9751805508837553, 0.972498435188347, 0.9738337437820925)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference on a Single Samples"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "dq_net = DeepQNet(dataset= data)\r\n",
    "dq_net.load_model(\"models/ids_dqnet.ckpt\")\r\n",
    "idx = 96142                                                                   # Getting a point from the dataset\r\n",
    "test_point = data.states_features[idx].reshape(1, data.feature_dim)           # Getting the features\r\n",
    "true_label = data.actions_labels[idx]                                         # Getting the true action\r\n",
    "print(test_point)                                                             # Printing the features\r\n",
    "out = dq_net.predict(test_point)                                              # Perform the feeding\r\n",
    "print(\"Our DQN predicted the action: {}\".format(out))       \r\n",
    "print(\"True (ground truth) action: {}\".format(true_label))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[4.00000000e+00 4.00000000e+00 1.83000000e+02 2.33000000e+02\n",
      "  9.00000000e+00 1.80000000e+01 3.00000000e+00 1.00000000e+01\n",
      "  3.00000000e+00 1.00000000e+01 0.00000000e+00 4.10000000e+01\n",
      "  1.90000000e+01 1.15000000e+02 2.00000000e-01 5.00000000e-01\n",
      "  1.00000000e+00 0.00000000e+00 7.00000000e+01 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.64367819e-01 1.09837307e+00]]\n",
      "WARNING:tensorflow:10 out of the last 73394 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA98173950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Our DQN predicted the action: 4\n",
      "True (ground truth) action: [4.]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "eded81c4a7c6917c9cbd3629f4297c0af6b02e3629b6d4cad1fc0bc42eaeccd9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}